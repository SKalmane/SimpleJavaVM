* Question 1: Application Startup
  Situation: The VM under-performs on a certain benchmark suite. The benchmark puts heavy weighting on the
  work performed in the first tens of seconds of the VM's run.
  Therefore, to improve the benchmark performance, need to improve early stage performance of the VM.
  What are the issues that affect VM startup time, and how might we design or configure our VM to minimize these
  issues?
    
* Answer 1: 
** 2. The VM takes as its input the class file and runs the program. 
** 3. Initially, the VM interprets the code. 
** 4. Template Interpreter is something that is constructed on the fly, when the program is waiting to start running.
      This can take up time in the beginning. Though it is more efficient than the Switch Interpreter, it can end up 
   causing an increase in the startup time - useful for server side applications which run for a long time, not 
   so much for short client side applications, where the startup time caused by the use of such an interpreter
   can end up mroe than offsetting the efficiency and steady state performance boost provided by the use of 
   such an interpreter. 
** Compiling code in the beginning increases startup time as well. Spreading out the compilation over time can help 
   offset this. 
** 5. Bootstrap class loader --> System class loader --> Context class loader. If a class not found amongst all three, 
   we throw ClassNotFoundException
** 6. ClassLoading has loading, linking and initialization. 
** 7. Generally, classloading algo is lazy. Only loads class files that need to be loaded. 
** 8. Upfront verification of the bytecode can also add to startup time. Verifying each bytecode speeds up startup time, 
   but makes each bytecode's execution slower. 
** 9. Class Hierarchy Analysis can take time as well. 
   

* Question 2: Memory management
** Has JNI, needs to be used on an embedded processor => Copying collector - bad idea.
** Processor in question: Single core processor with 2 threads
** Relatively small main memory => we need good throughput i.e., faster cycles of GC 
   to clean up memory
** Single level of cache
** Relatively long latency between main memory and cache => We would want good cache 
   performance to limit the number of times we need to get instructions from cache
** This is a server application => Pause times not a concern
** 
 
   
* Answer 2: 
  

* Question 3: Refactoring (125 points)
You are the tech lead for a development group that has built a large Java application. Application performance is
becoming a major issue, and your management has set aside some time in your release schedule to make perfor-
mance improvements. At a brainstorming meeting, members of your team suggest using the time in the following
ways:
1. Refactor the code so that it interacts with as many classes as possible early in the application’s run, forcing the
class loader to load all classes early. - Loading the classes early would ensure that the chances of dynamic 
class loading would reduce. With the reduction in dynamic class loading, the Class Hierarchy Analysis that we do 
(and base our JIT compilation on) would be more accurate in its optimizations and this could lead to better performance.
Also, having a class load later could make some of the assumptions we made to carry out our JIT compilation incorrect, 
requiring the VM to backtrack and run the code in interpreted mode. This could lead to quite some performance degradation,
since now not only do we need to step back to a previous state (negating the time taken in executing the compiled code), 
but we would also be running the code in interpreted mode, which would be a lot slower than the JIT compiled version. 

2. Minimize the use of object orientation, in order to maximize the number of static method calls. - With the use of object
oriented concepts, we can get to a place where we have a tall Class Hierarchy, with lots of class inheritance and method
overriding. Having a complicated Class Hierarchy could possibly result in a lot of polymorphic method calls. This would be 
less efficient as dynamic dispatch would be needed (we would need to look through our CHA and figure out the right runtime 
object to which the method belongs).  Maximizing the number of static method calls would therefore mean that we have fewer 
polymorphic calls. This would result in lots more occurrences of invokestatic in the generated bytecode and since
this would need single dispatch, the assumption is that the performance will be a lot better.
While this seems like a plausible idea, there are some caveats here - the assumption here is that we minimize the use of 
object orientation while not increasing the complexity or code size of our program i.e., we still have approximately the 
same(or lesser) number of method calls and the same(or lesser) approximate number of operations/allocations/deallocations
etc. Because, quite possibly, the use of static method calls in place of polymorphic method calls could result in increased
complexity since now we would need more classes to do our work (or more methods in the same classes to differentiate the 
work to be done) - this added complexity might negate the benefits gained by using static method calls. 
Also, not to be forgotten, reducing the use of object orientation could lead to less clear, less readable code - and this 
would be a negative that also needs to be taken into account. 

3. Re-implement parts of the code in a native language and integrate with the VM using JNI - We could choose to implement
part of our code in a native language if there were potential benefits that the platform offered which the native code
could exploit. Also, if there was some critical piece of code which is being called numerous times, implementing it in 
native code (or handcoding it in assembly) could make it perform better. 
However, there are potential pitfalls to integrating native code with our Java application. Garbage collection becomes
more complicated since we need to take care of pointers (and the deallocation of objects allocated by native code). 

4. Implement an object pooling strategy that pre-allocates and re-uses objects rather than creating new instances
on demand. - Reusing objects would mean that we pass around the object references instead of creating new objects 
on the fly. Depending upon the garbage collector and the allocation mechanism being used, this could actually end
up hurting performance. For instance, let's consider the case where we are using a semi-space copying collector, where 
we copy the live objects to a different region in every GC cycle. In this case, if we have objects that have been 
allocated once and are then alive for a longer duration, we will need to keep copying them from one semi-space to 
another, increasing the overhead for every GC. This would also reduce the amount of garbage objects in every cycle, 
therefore resulting in more work in every cycle. 
On the other hand, if we happen to use a generational collector, the re-used objects would probably end up in the 
mature space and would not be touched until the major GC cycle. There would be a lot more live objects (and hence
more objects end up in the mature space) but there would definitely be lesser allocations (though with Bump Pointer
allocation, each allocation doesn't add too much overhead) and this MAY provide some performance boost. 
 
5. Reduce the size of synchronized sections, and replace explicit monitor acquisitions with atomic operations.
This question has two parts:
(a) Expand on each of the proposals, describing the rationale behind each and give your opinion on whether it is a
useful optimization.
(b) You (quite reasonably) decide that you don’t have enough information to make a choice. Describe what data you
would like your team to gather by profiling the application in order to make an informed decision.
  
* Question 4: Highly concurrent JVM 
You are designing a new custom Java Virtual Machine that aims to maximize performance for highly concurrent
applications. You will be targeting large server machines that have lots of memory and many hardware threads.
However, you expect that the applications that will target your VM will have many more Java threads than the server
has hardware threads.
2You want to optimize the VM for concurrent throughput, even at the expense of single-thread performance. You must
implement the JVM specification (i.e. you cannot change the memory model, and your system must implement priority
scheduling). For each of the following components, discuss any design decisions that you might make to emphasize
multi-threaded performance:
(a) The implementation of monitors, including the contention manager that determines which thread should obtain a
monitor or be scheduled from a set of waiting threads.
(b) The object model for heap-allocated objects and arrays, including the per-object locking strategy.
(c) The scheduling mechanism.

* Question 5: Decompiling
A decompiler is a computer program that attempts to recreate the original source code for an application by examining
its executable or (as in the case of Java) its bytecode. Decompilation for Java is a mature field of study, and commercial
decompilers exist that can do an impressive job of regenerating lost source code. Part of their implementation involves
pattern recognition (Java compilers generally emit similar code for similar structures; we saw some of these patterns
in class, including the bytecode structure of a loop or a finally block). However, the majority of their information
comes from the debug data stored in the class file.
Java bytecode lends itself well to decompilation, but this is not necessarily an advantage for some application de-
velopers. Consider proprietary commercial software that is distributed as a .jar file. Such code may contain trade
secret algorithms, or implementations of proprietary protocols or file formats. Unfortunately for the authors of such
software it is impossible to completely defeat decompilers (the same is true for native code, although decompiling an
executable is far harder than a .jar file). However that does not mean that developers have to make life easy for
those who decompile their code.
Imagine that you were to build an application that post-processes valid Java class files to make them harder to decom-
pile. Propose a series of transformations that you would make to the class file, with the following conditions:
• The resulting class file must still be valid, according to the JVM specification.
• The semantics of the program must not change (i.e. your modifications should not affect the behavior of the
original application).
• You should not modify the Code attribute of a method.
Beyond those constraints you are free to rewrite the class files however you wish in order to make them harder to
decompile (or to make them less useful once they have inevitably been decompiled). Be explicit - for this question
you should refer to Chapter 4 of the Java Virtual Machine specification
What we can do is the following - Change the version number of the file to be a later version (let's say our code
is designed to run in version 49, but we change the version number to 51.) Then, we add valid data to the attributes
table for many of the attributes which are not supported in the version it's designed for, but it is supported in 
the version we changed the class file to. So this would now result in valid, different code when decompiled. The more
creative we can get with modifying the attribute table which are supported in version B but not in version A, the more
different the decompiled code will be. To have our class file work as before, we would need to change the version number
back to our original, designed version and it would work correctly. 
